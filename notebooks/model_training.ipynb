import pandas as pd 
from sklearn.model_selec on import train_test_split 
from sklearn.preprocessing import LabelEncoder, StandardScaler 
from sklearn.metrics import accuracy_score, classifica on_report, confusion_matrix, 
precision_recall_fscore_support 
from sklearn.ensemble import RandomForestClassifier, GradientBoos ngClassifier 
from sklearn.linear_model import Logis cRegression 
from sklearn.svm import LinearSVC 
from sklearn.dummy import DummyClassifier 
from sklearn.u ls.class_weight import compute_sample_weight 
import lightgbm as lgb 
import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns 

# Load dataset 
data = pd.read_csv("lung_cancer_predic on.csv") 
print("Dataset shape:", data.shape) 
print("\nFirst few rows:") 
print(data.head()) 
# Display class balance BEFORE encoding 
print("\nClass Distribu on:") 
print(data['Final_Predic on'].value_counts(), '\n')  
 
target_col = 'Final_Predic on' 
X = data.drop(columns=[target_col]) 
y = data[target_col] 
 
# Encode target variable 
le_target = LabelEncoder() 
y_encoded = le_target.fit_transform(y) 
 
print(f"Target classes: {le_target.classes_}") 
print(f"Encoded target distribu on: {np.bincount(y_encoded)}\n") 
 
X_train, X_test, y_train, y_test = train_test_split( 
    X, y_encoded, test_size=0.2, random_state=42, stra fy=y_encoded 
) 
 
# encode categorical features for train and test 
categorical_cols = X_train.select_dtypes(include=['object']).columns 
label_encoders = {} 
 
for col in categorical_cols: 
    le = LabelEncoder() 
    # Fit on training data only 
    X_train[col] = le.fit_transform(X_train[col].astype(str)) 
    # Store encoder for test set 
    label_encoders[col] = le 
    # Transform test set, handling unseen categories 
    X_test[col] = X_test[col].astype(str).apply( 
        lambda x: le.transform([x])[0] if x in le.classes_ else -1 
    ) 
 
# Convert to numpy arrays to avoid warnings 
X_train = X_train.values 
X_test = X_test.values 
print(f"Training set shape: {X_train.shape}") 
print(f"Test set shape: {X_test.shape}") 
print(f"Training target distribu on: {np.bincount(y_train)}") 
print(f"Test target distribu on: {np.bincount(y_test)}") 
print(f"Class imbalance ra o: {np.bincount(y_train)[0] / np.bincount(y_train)[1]:.2f}:1\n") 
# Scale features for models that require it 
scaler = StandardScaler() 
X_train_scaled = scaler.fit_transform(X_train) 
X_test_scaled = scaler.transform(X_test) 

# Calculate sample weights for class imbalance (for Gradient Boos ng) 
sample_weights = compute_sample_weight( 
class_weight='balanced', 
y=y_train 
) 
# Baseline model for reference 
print("="*60) 
print("BASELINE MODEL") 
print("="*60) 
baseline = DummyClassifier(strategy="most_frequent") 
baseline.fit(X_train, y_train) 
base_preds = baseline.predict(X_test) 
print("Baseline accuracy (most frequent class):", accuracy_score(y_test, base_preds)) 
print("\nClassifica on Report:") 
print(classifica on_report(y_test, base_preds, target_names=le_target.classes_)) 
 
# Define the models to compare with IMPROVED hyperparameters 
models = { 
    "Logis c Regression": Logis cRegression( 
        max_iter=1000, 
        random_state=42, 
        class_weight='balanced' 
    ), 
    "Linear SVM": LinearSVC( 
        max_iter=2000, 
        dual=False, 
        random_state=42, 
        class_weight='balanced' 
    ), 
    "Random Forest": RandomForestClassifier( 
        n_esmators=200, 
        max_depth=15, 
        min_samples_split=5, 
        min_samples_leaf=2, 
        random_state=42, 
        n_jobs=-1, 
        class_weight='balanced' 
    ), 
    "Gradient Boos ng": GradientBoos ngClassifier( 
        n_esmators=200, 
        max_depth=7, 
        learning_rate=0.05, 
        min_samples_split=10, 
        min_samples_leaf=5, 
        subsample=0.8, 
        random_state=42 
    ), 
    "LightGBM": lgb.LGBMClassifier( 
        n_esmators=200, 
        max_depth=7, 
        learning_rate=0.05, 
        num_leaves=31, 
        random_state=42, 
        class_weight='balanced', 
        verbose=-1 
    ) 
} 
results = {} 
metrics_dict = {'Model': [], 'Precision': [], 'Recall': [], 'F1-Score': []} 
 
# Evaluate all models with full reporting and store metrics for plots 
for name, model in models.items(): 
    print("\n" + "="*60) 
    print(f"{name}") 
    print("="*60) 
 
    try: 
        if name in ['Logis c Regression', 'Linear SVM']: 
            model.fit(X_train_scaled, y_train) 
            preds = model.predict(X_test_scaled) 
        elif name == 'Gradient Boos ng': 
            # Use sample weights for Gradient Boos ng 
            model.fit(X_train, y_train, sample_weight=sample_weights) 
            preds = model.predict(X_test) 
        else: 
            model.fit(X_train, y_train) 
            preds = model.predict(X_test) 
 
        acc = accuracy_score(y_test, preds) 
        results[name] = acc 
 
        print(f"Accuracy: {acc:.4f}") 
        print("\nConfusion Matrix:") 
        print(confusion_matrix(y_test, preds)) 
        print("\nClassifica on Report:") 
        print(classifica on_report(y_test, preds, target_names=le_target.classes_)) 
 
        precision, recall, f1, _ = precision_recall_fscore_support(y_test, preds, average='weighted') 
        metrics_dict['Model'].append(name) 
        metrics_dict['Precision'].append(precision) 
        metrics_dict['Recall'].append(recall) 
        metrics_dict['F1-Score'].append(f1) 
 
    except Excep on as e: 
        print(f"Error training {name}: {str(e)}") 
        results[name] = 0.0 
        metrics_dict['Model'].append(name) 
        metrics_dict['Precision'].append(0) 
        metrics_dict['Recall'].append(0) 
        metrics_dict['F1-Score'].append(0) 
 
#results 
print("\n" + "="*60) 
print("SUMMARY OF ALL MODELS") 
print("="*60) 
results_df = pd.DataFrame(list(results.items()), columns=['Model', 'Accuracy']) 
results_df = results_df.sort_values('Accuracy', ascending=False) 
print(results_df.to_string(index=False)) 
print(f"\nBest Model: {results_df.iloc[0]['Model']} with accuracy: {results_df.iloc[0]['Accuracy']:.4f}") 
# Save the best model for later use 
best_model_name = results_df.iloc[0]['Model'] 
best_model = models[best_model_name] 
# Plot Accuracy Comparison 
plt.figure(figsize=(10,6)) 
ax = sns.barplot(x='Accuracy', y='Model', data=results_df, pale e='viridis') 
plt. tle('Model Accuracy Comparison') 
plt.xlabel('Accuracy') 
plt.ylabel('Model') 
plt.xlim(0, 1) 
# Annotate bars with accuracy values 
for p in ax.patches: 
width = p.get_width() 
y = p.get_y() + p.get_height() / 2 
ax.text(width + 0.01, y, f'{width:.4f}', va='center', fontsize=9) 
plt. ght_layout() 
plt.show() 
# Plot Precision, Recall, F1-Score Comparison 
metrics_df = pd.DataFrame(metrics_dict).sort_values('F1-Score', ascending=True) 
bar_width = 0.25 
r1 = np.arange(len(metrics_df)) 
r2 = [x + bar_width for x in r1] 
40 
r3 = [x + bar_width for x in r2] 
plt.figure(figsize=(12,7)) 
plt.bar(r1, metrics_df['Precision'], width=bar_width, label='Precision', color='skyblue') 
plt.bar(r2, metrics_df['Recall'], width=bar_width, label='Recall', color='lightgreen') 
plt.bar(r3, metrics_df['F1-Score'], width=bar_width, label='F1-Score', color='salmon') 
plt.x cks([r + bar_width for r in range(len(metrics_df))], metrics_df['Model'], rota on=45) 
plt.ylabel('Score') 
plt.ylim(0, 1) 
plt. tle('Precision, Recall, and F1-Score Comparison Across Models') 
plt.legend() 
plt. ght_layout() 
plt.show() 
# Plot confusion matrix of the best model 
if best_model_name in ['Logis c Regression', 'Linear SVM']: 
y_pred_best = best_model.predict(X_test_scaled) 
else: 
y_pred_best = best_model.predict(X_test) 
cm = confusion_matrix(y_test, y_pred_best) 
plt.figure(figsize=(6,5)) 
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', x cklabels=le_target.classes_, 
y cklabels=le_target.classes_) 
plt.ylabel('Actual') 
plt.xlabel('Predicted') 
plt. tle(f'Confusion Matrix of Best Model: {best_model_name}') 
plt.show() 
print("\n 
Training complete! Models saved for Gradio interface.")
